# Human-in-the-loop Reinforcement Learning for Emotion Recognition
Introduces a enhanced approach called two-state Q-learning with human feedback (TS-QL-HF) that uses Q-learning and human feedback to improve the accuracy of facial emotion recognition systems without requiring tramendous amount of data on minority population, ensuring fairness when building such systems.

![FER System](https://github.com/tansweeyang/Human-in-the-loop-Reinforcement-Learning-for-Emotion-Recognition/blob/ba6fc30777812190ac3a97de65332fbf957dfd28/FER_Flow.jpg)

## Publications
- Research paper: http://example.com/file.pdf
- Final year project report: https://github.com/tansweeyang/An-Emotion-Recognition-System-Integrated-with-Human-Intelligence/blob/75cbc1d33f7cc33245475dbf1f9e036436d7afe1/documents/SE_1904180_FYP%20report%20-%20TanSweeYang.pdf

## Dataset Used
https://www.v7labs.com/open-datasets/ferg

## Key Contributions
- Design TS-QL-HF, which incorporates human feedback as an alternative reward function for TS-QL, for enhancing the quality of rewards
- Develop an action selection strategy and fine-tune hyperparameters for improving the convergence rate while operating in deterministic environments
- Separate the learning of unrelated policies among multiple agents for improving convergence
- Investigate the use of DQL in deterministic environments.

## Results
![Results](https://github.com/tansweeyang/Human-in-the-loop-Reinforcement-Learning-for-Emotion-Recognition/blob/98b84ab686210d67d84b11fb996dbc64103e4fbe/Results.jpg)
